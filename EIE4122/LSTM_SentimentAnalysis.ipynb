{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lXapSRrg2-Bf"},"source":["#**This Colab is based on the Colab file in [link](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb). To reduce running time, we will use a reduced-size IMDb dataset.**"]},{"cell_type":"markdown","metadata":{"id":"rTDQOnRQ2Pd7"},"source":["# **Advanced Sentiment Analysis**\n","\n","We have learned the fundamentals of sentiment analysis in `RNN_SentimentAnalysis.ipynb`. In this notebook, we'll use advanced techniques and models to obtain better results. We will use:\n","- packed padded sequences\n","- bidirectional LSTM\n","- multi-layer LSTM\n","- regularization\n","- Adam optimizer\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t7bWsN9_2Pd_"},"source":["## **1. Preparing Data**\n","\n","As before, we'll set the seed, define the `Fields` and get the train/valid/test splits.\n","\n","We'll be using *packed padded sequences*, which will make the LSTM processes the non-padded elements of our sequence only. This will speed up computtion. To use packed padded sequences, we have to tell the LSTM how long the actual sequences are. We do this by setting `include_lengths = True` in our `TEXT` field. This will cause `batch.text` to be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4gU0qNY3ha_","outputId":"aa56d0b1-df00-4f41-c280-03500dae884b","executionInfo":{"status":"ok","timestamp":1667033549697,"user_tz":-480,"elapsed":30519,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5bPMlHJ3pzN","outputId":"a2c3c6b5-4724-48b1-e0e7-f9b508554370","executionInfo":{"status":"ok","timestamp":1667033553584,"user_tz":-480,"elapsed":490,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["# Create a working folder and cd to it.\n","!mkdir -p /content/drive/MyDrive/Learning/EIE4122/lab2\n","%cd /content/drive/MyDrive/Learning/EIE4122/lab2"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Learning/EIE4122/lab2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dr8j50_DMzBd","outputId":"eb3002fa-0833-44c0-dbf7-c8bad0b3156f","executionInfo":{"status":"ok","timestamp":1667033563677,"user_tz":-480,"elapsed":4462,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["# Require gdown to download files from Google Drive\n","!pip install gdown"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}]},{"cell_type":"code","metadata":{"id":"u3_k11-MM8YP"},"source":["# Download the reduced-size IMDB dataset\n","!if [ ! -f data.tgz ]; then gdown https://drive.google.com/uc?id=1EL6YlIEs1IbUDjrZvrZ_9-C0C4gYoryZ; fi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-t8CNiBNYIg","outputId":"460e204d-b643-40fa-d36c-b0294c510878"},"source":["!echo \"Start decompression. It will take few minutes\"\n","!tar xf data.tgz --exclude=\"._*\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start decompression. It will take few minutes\n"]}]},{"cell_type":"code","source":["# Install torchtext. Note that if this step asks you to reset the runtime, you will need to remount your Google Drive.\n","!pip install torchtext==0.11.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642},"id":"Q9oQkR0Q_0j3","outputId":"001df509-29c4-48c7-c29a-5a2baff0d3da","executionInfo":{"status":"ok","timestamp":1667033215002,"user_tz":-480,"elapsed":110131,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.11.0\n","  Downloading torchtext-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n","\u001b[K     |████████████████████████████████| 8.0 MB 32.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\n","Collecting torch==1.10.0\n","  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.1 MB/s eta 0:00:42tcmalloc: large alloc 1147494400 bytes == 0x3a142000 @  0x7fd61ea25615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n","\u001b[K     |████████████████████████████████| 881.9 MB 20 kB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (4.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n","Installing collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.13.1\n","    Uninstalling torchtext-0.13.1:\n","      Successfully uninstalled torchtext-0.13.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0 torchtext-0.11.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchtext"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"5dtj8e002Pd_","executionInfo":{"status":"ok","timestamp":1667033470553,"user_tz":-480,"elapsed":13459,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["import torch\n","from torchtext.legacy import data\n","\n","SEED = 1234\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","TEXT = data.Field(tokenize = 'spacy',\n","                  tokenizer_language = 'en_core_web_sm',\n","                  include_lengths = True)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFljduy82PeB"},"source":["We then load the IMDb dataset."]},{"cell_type":"code","metadata":{"id":"0IwuTSs92PeC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46fdd554-049e-47a1-f5ba-9473a860c66d","executionInfo":{"status":"ok","timestamp":1667033712118,"user_tz":-480,"elapsed":135470,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["# This step should be quick as PyTorch only need to know the structure of the data in the\n","# folder \".data/imdb/aclImdb\". It will not actually load the data, which will be done by\n","# the Dataloader object (see code below)\n","print('Loading data from .data/imdb/aclImdb')\n","from torchtext.legacy import datasets\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, path='.data/imdb/aclImdb/')\n","print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data from .data/imdb/aclImdb\n","Number of training examples: 2780\n","Number of testing examples: 2780\n"]}]},{"cell_type":"markdown","metadata":{"id":"walMdSTD2PeC"},"source":["Then create the validation set from our training set."]},{"cell_type":"code","metadata":{"id":"xxAmfAch2PeD","executionInfo":{"status":"ok","timestamp":1667034511528,"user_tz":-480,"elapsed":513,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["import random\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PBZdvQAt2PeD"},"source":["We will not use the pretrained word embedding as suggested in [link](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb) despite its superior performance. The pretrained embedding model uses too much disk space and takes too long to load. Instead, we will use the same word embedding as \"RNN_Sentiment Analysis.ipynb\"."]},{"cell_type":"code","metadata":{"id":"UvQjlZfI2PeE","executionInfo":{"status":"ok","timestamp":1667034515625,"user_tz":-480,"elapsed":509,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["MAX_VOCAB_SIZE = 25_000\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE)\n","#                 vectors = \"glove.6B.100d\",          # Do not use complicated word-embedding to reduce download time\n","#                 unk_init = torch.Tensor.normal_)\n","LABEL.build_vocab(train_data)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xiFf3kS2PeE"},"source":["As before, we create the iterators, placing the tensors on the GPU if one is available.\n","\n","Another thing for packed padded sequences is that all of the tensors within a batch need to be sorted by their lengths. To instruct the iterator to perform sorting, we set `sort_within_batch = True`."]},{"cell_type":"code","metadata":{"id":"XX-OUZKt2PeE","executionInfo":{"status":"ok","timestamp":1667034520203,"user_tz":-480,"elapsed":3,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["BATCH_SIZE = 64\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    device = device)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"63QkigvZ2PeG","executionInfo":{"status":"ok","timestamp":1667034525140,"user_tz":-480,"elapsed":543,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):        \n","        super().__init__()        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)        \n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)        \n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text, text_lengths):        \n","        #text = [sent len, batch size]\n","\n","        embedded = self.dropout(self.embedding(text))        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        #pack sequence, lengths need to be on CPU!\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","        \n","        #unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bg-W0U9N2PeG"},"source":["Like before, we'll create an instance of our RNN class, with the new parameters and arguments for the number of layers, bidirectionality and dropout probability.\n","\n","We get our pad token index from the vocabulary, getting the actual string representing the pad token from the field's `pad_token` attribute, which is `<pad>` by default."]},{"cell_type":"code","metadata":{"id":"AB-mNIwo2PeG","executionInfo":{"status":"ok","timestamp":1667034531600,"user_tz":-480,"elapsed":746,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bgOvmE962PeF"},"source":["## **2. Build the Model**\n","\n","The model features the most drastic changes.\n","\n","### Different RNN Architecture\n","\n","We'll be using a different RNN architecture called a Long Short-Term Memory (LSTM). Why is an LSTM better than a standard RNN? Standard RNNs suffer from the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). LSTMs overcome this by having an extra recurrent state called a _cell_, which can be thought of as the \"memory\" of the LSTM. The LSTM uses multiple _gates_ which control the flow of information into and out of the memory. For more information, go [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). We can simply think of the LSTM as a function of $x_t$, $h_t$ and $c_t$, instead of just $x_t$ and $h_t$.\n","\n","$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n","\n","Thus, the model using an LSTM looks something like (with the embedding layers omitted):\n","\n","![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment2.png?raw=1)\n","\n","The initial cell state $c_0$, like the initial hidden state, is initialized to a tensor of all zeros. The sentiment prediction is still, however, only made using the final hidden state, not the final cell state, i.e. $\\hat{y}=\\sigma({\\bf w}_o^\\top h_T)$.\n","\n","### Bidirectional RNN\n","\n","The concept behind a bidirectional RNN is simple. As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$. \n","\n","In PyTorch, the hidden state (and cell state) tensors returned by the forward and backward RNNs are stacked on top of each other in a single tensor. \n","\n","We make our sentiment prediction using a concatenation of the last hidden state from the forward RNN (obtained from final word of the sentence), $h_T^\\rightarrow$, and the last hidden state from the backward RNN (obtained from the first word of the sentence), $h_T^\\leftarrow$, i.e. $\\hat{y}=\\sigma({\\bf w}_o^\\top[h_T^\\rightarrow, h_T^\\leftarrow])$   \n","\n","The image below shows a bi-directional RNN, with the forward RNN in orange, the backward RNN in green and the linear layer in silver.  \n","\n","![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment3.png?raw=1)\n","\n","### Multi-layer RNN\n","\n","Multi-layer RNNs (also called *deep RNNs*) are another simple concept. The idea is that we add additional RNNs on top of the initial standard RNN, where each RNN added is another *layer*. The hidden state output by the first (bottom) RNN at time-step $t$ will be the input to the RNN above it at time step $t$. The prediction is then made from the final hidden state of the final (highest) layer.\n","\n","The image below shows a multi-layer unidirectional RNN, where the layer number is given as a superscript. Also note that each layer needs their own initial hidden state, $h_0^L$.\n","\n","![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment4.png?raw=1)\n","\n","### Regularization\n","\n","Although we've added improvements to our model, each one adds additional parameters, which may cause overfitting. The more parameters you have in in your model, the higher the probability that your model will overfit (memorize the training data, causing  a low training error but high validation/testing error, i.e., poor generalization to new, unseen examples). To combat this, we use regularization. More specifically, we use a method of regularization called *dropout*. Dropout works by randomly *dropping out* some nodes, i.e., setting the output of some neurons to 0 during a forward pass. The probability that each neuron is dropped out is set by a hyperparameter, and each neuron is considered indepenently. One theory about why dropout works is that a model with parameters dropped out can be seen as a \"weaker\" (less parameters) model. The predictions from all of these \"weaker\" models (one for each forward pass) get averaged together within the parameters of the model. Thus, your model can be thought of as an ensemble of weaker models, none of which are over-parameterized and thus should not overfit.\n","\n","### Implementation Details\n","\n","Another addition to this model is that we are not going to learn the embedding for the `<pad>` token. This is because we want to explitictly tell our model that padding tokens are irrelevant to determining the sentiment of a sentence. This means the embedding for the pad token will remain at what it is initialized to (we initialize it to all zeros later). We do this by passing the index of our pad token as the `padding_idx` argument to the `nn.Embedding` layer.\n","\n","To use an LSTM instead of the standard RNN, we use `nn.LSTM` instead of `nn.RNN`. Also, note that the LSTM returns the `output` AND a tuple containing the final `hidden` state and the final `cell` state, whereas the standard RNN returned the `output` and final `hidden` state only. \n","\n","The final hidden state of our LSTM has both a forward and backward components. Because these components will be concatenated together, the size of the input to the `nn.Linear` layer is twice that of the hidden dimension.\n","\n","Implementing bidirectionality and adding additional layers are done by passing values for the `num_layers` and `bidirectional` arguments to the LSTM. \n","\n","Dropout is implemented by initializing an `nn.Dropout` layer (the argument is the probability of dropping out each neuron) and using it within the `forward` method. The layer can be added after each layer we want to apply dropout. **Note**: never use dropout on the input or output layers (`text` or `fc` in this case), you only ever want to use dropout on intermediate layers. The LSTM has a `dropout` argument which adds dropout on the connections between hidden states in one layer to hidden states in the next layer. \n","\n","As we are passing the lengths of our sentences so that packed padded sequences can be used, we have to add a second argument, `text_lengths`, to `forward`. \n","\n","Before we pass our embeddings to the LSTM layer, we need to pack them. We do this using tbe `nn.utils.rnn.packed_padded_sequence`. This will cause our network to only process the non-padded elements of our sequence. The network will then return `packed_output` (a packed sequence) as well as the `hidden` and `cell` states (both of which are tensors). Without packed padded sequences, `hidden` and `cell` are tensors from the last element in the sequence, which will most probably be a pad token. However when using packed padded sequences, they are both from the last non-padded element in the sequence. Note that the `lengths` argument of `packed_padded_sequence` must be a CPU tensor. To achieve this, we use `.to('cpu')`. More information about `packed_padded_sequence` can be found here [link](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch)\n","\n","We then unpack the output sequence, using `nn.utils.rnn.pad_packed_sequence`, to transform it from a packed sequence to a tensor. The elements of `output` from padding tokens will be zero tensors (tensors where every element is zero). Usually, we only have to unpack output if we are going to use it later. Although we aren't in this case, we still unpack the sequence just to show how it is done.\n","\n","The final hidden state, `hidden`, has a shape of _**[num layers * num directions, batch size, hid dim]**_. These are ordered: **[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]**. As we want the final (top) layer forward and backward hidden states, we get the top two hidden layers from the first dimension, `hidden[-2,:,:]` and `hidden[-1,:,:]`, and concatenate them together before passing them to the linear layer (after applying dropout). "]},{"cell_type":"markdown","metadata":{"id":"MUzhgWv72PeH"},"source":["We'll print out the number of parameters in our model. \n","\n","Notice how we have almost twice as many parameters as before!"]},{"cell_type":"code","metadata":{"id":"pCpa4Xti2PeH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6c3238d-192b-4333-ae94-316c45354226","executionInfo":{"status":"ok","timestamp":1667034566211,"user_tz":-480,"elapsed":518,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 4,810,857 trainable parameters\n"]}]},{"cell_type":"markdown","metadata":{"id":"v9MEwGlj2PeJ"},"source":["We then replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n","\n","**Note**: this should always be done on the `weight.data` and not the `weight`!"]},{"cell_type":"markdown","metadata":{"id":"NZIDAI4R2PeJ"},"source":["As our `<unk>` and `<pad>` token are irrelevant to sentiment, it is preferable to initialize them to all zeros to indicate that they are irrelevant. We achieve this by manually setting their row in the embedding weights matrix to zeros. We get their row by finding the index of the tokens, which we have already done for the padding index.\n","\n","**Note**: like initializing the embeddings, this should be done on the `weight.data` and not the `weight`."]},{"cell_type":"code","metadata":{"id":"QOx4XRPv2PeK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb7bd8c8-4294-4175-ae72-c5fa74b2f6f7","executionInfo":{"status":"ok","timestamp":1667034615199,"user_tz":-480,"elapsed":454,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)\n","print(model.embedding.weight.data.size())"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.7289, -0.7336,  1.5624,  ..., -0.5592, -0.4480, -0.6476],\n","        ...,\n","        [ 0.0914,  1.5196,  0.4670,  ...,  0.6393, -0.0332,  0.0185],\n","        [-0.6290,  0.4650, -0.7165,  ..., -1.3171,  2.0381, -2.0497],\n","        [-1.1222, -0.0240, -1.0878,  ..., -0.4948, -0.3874,  0.0339]])\n","torch.Size([25002, 100])\n"]}]},{"cell_type":"markdown","metadata":{"id":"UdHjPAcP2PeK"},"source":["We can now see the first two rows of the embedding weights matrix have been set to zeros. As we passed the index of the pad token to the `padding_idx` of the embedding layer it will remain zeros throughout training, however the `<unk>` token embedding will be learned."]},{"cell_type":"markdown","metadata":{"id":"1oxW-3Hp2PeK"},"source":["## Train the Model"]},{"cell_type":"markdown","metadata":{"id":"j3_2xj-s2PeL"},"source":["The only change we'll make here is changing the optimizer from `SGD` to `Adam`. SGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. `Adam` adapts the learning rate for each parameter, giving parameters that are updated more frequently lower learning rates and parameters that are updated infrequently higher learning rates. More information about `Adam` (and other optimizers) can be found [here](http://ruder.io/optimizing-gradient-descent/index.html).\n","\n","To change `SGD` to `Adam`, we simply change `optim.SGD` to `optim.Adam`, also note how we do not have to provide an initial learning rate for Adam as PyTorch specifies a sensibile default initial learning rate."]},{"cell_type":"code","metadata":{"id":"7WcsJQex2PeL","executionInfo":{"status":"ok","timestamp":1667034662261,"user_tz":-480,"elapsed":3,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["import torch.optim as optim\n","optimizer = optim.Adam(model.parameters())"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qhx4nc6u2PeL"},"source":["The rest of the steps for training the model are unchanged. We define the criterion and place the model and criterion on the GPU (if available)..."]},{"cell_type":"code","metadata":{"id":"oveVNuyb2PeL","executionInfo":{"status":"ok","timestamp":1667034669837,"user_tz":-480,"elapsed":4098,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["criterion = nn.BCEWithLogitsLoss()\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-0TH7PO92PeM"},"source":["We implement the function to calculate accuracy..."]},{"cell_type":"code","metadata":{"id":"hcWrFKlt2PeM","executionInfo":{"status":"ok","timestamp":1667034674086,"user_tz":-480,"elapsed":461,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HBIiVdiH2PeN"},"source":["We define a function for training our model. \n","\n","As we have set `include_lengths = True`, our `batch.text` is now a tuple with the first element being the numericalized tensor and the second element being the actual lengths of each sequence. We separate these into their own variables, `text` and `text_lengths`, before passing them to the model.\n","\n","**Note**: as we are now using dropout, we must remember to use `model.train()` to ensure the dropout is \"turned on\" while training."]},{"cell_type":"code","metadata":{"id":"io86dBF-2PeO","executionInfo":{"status":"ok","timestamp":1667034698202,"user_tz":-480,"elapsed":3,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0    \n","    model.train()\n","    \n","    for batch in iterator:        \n","        optimizer.zero_grad()        \n","        text, text_lengths = batch.text        \n","        predictions = model(text, text_lengths).squeeze(1)\n","        loss = criterion(predictions, batch.label)\n","        acc = binary_accuracy(predictions, batch.label)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QL48m4kO2PeP"},"source":["Then we define a function for testing our model, again remembering to separate `batch.text`.\n","\n","**Note**: as we are now using dropout, we must remember to use `model.eval()` to ensure the dropout is \"turned off\" while evaluating."]},{"cell_type":"code","metadata":{"id":"0amSnkph2PeR","executionInfo":{"status":"ok","timestamp":1667034703934,"user_tz":-480,"elapsed":809,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for batch in iterator:\n","            text, text_lengths = batch.text \n","            predictions = model(text, text_lengths).squeeze(1)\n","            loss = criterion(predictions, batch.label)\n","            acc = binary_accuracy(predictions, batch.label)\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"js3pzbyi2PeR"},"source":["And also create a nice function to tell us how long our epochs are taking."]},{"cell_type":"code","metadata":{"id":"0twmWuOW2PeS","executionInfo":{"status":"ok","timestamp":1667034707751,"user_tz":-480,"elapsed":4,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wFaUZH3k2PeS"},"source":["Finally, we train our model..."]},{"cell_type":"code","metadata":{"id":"y4mp8CZb2PeS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5cbc4386-376c-46d4-c72f-98a9990d9ea1","executionInfo":{"status":"ok","timestamp":1667034926413,"user_tz":-480,"elapsed":22331,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["N_EPOCHS = 5\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut2-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.565 | Train Acc: 71.07%\n","\t Val. Loss: 0.607 |  Val. Acc: 68.19%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.523 | Train Acc: 75.06%\n","\t Val. Loss: 0.615 |  Val. Acc: 68.42%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.477 | Train Acc: 76.97%\n","\t Val. Loss: 0.867 |  Val. Acc: 64.51%\n","Epoch: 04 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.445 | Train Acc: 79.17%\n","\t Val. Loss: 0.632 |  Val. Acc: 69.31%\n","Epoch: 05 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.405 | Train Acc: 82.01%\n","\t Val. Loss: 0.740 |  Val. Acc: 68.75%\n"]}]},{"cell_type":"markdown","metadata":{"id":"4FHD2EPw2PeS"},"source":["...and get our new and improved test accuracy!"]},{"cell_type":"code","metadata":{"id":"ZD5spO-l2PeT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a87049d4-16d5-4849-a6b4-1cc2df64d75a","executionInfo":{"status":"ok","timestamp":1667034741912,"user_tz":-480,"elapsed":2817,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["model.load_state_dict(torch.load('tut2-model.pt'))\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.702 | Test Acc: 61.14%\n"]}]},{"cell_type":"markdown","metadata":{"id":"kIBgdm2d2PeT"},"source":["## **3. User Input**\n","\n","We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n","\n","When using a model for inference it should always be in evaluation mode. If the above lab procedures are followed step-by-step, then the network should already be in evaluation mode (from doing `evaluate` on the test set), however we explicitly set it to avoid any risk.\n","\n","Our `predict_sentiment` function does a few things:\n","- sets the model to evaluation mode\n","- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n","- indexes the tokens by converting them into their integer representation from our vocabulary\n","- gets the length of our sequence\n","- converts the indexes, which are a Python list into a PyTorch tensor\n","- add a batch dimension by `unsqueeze`ing \n","- converts the length into a tensor\n","- squashes the output prediction from a real number between 0 and 1 with the `sigmoid` function\n","- converts the tensor holding a single value into an integer with the `item()` method\n","\n","We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."]},{"cell_type":"code","metadata":{"id":"tpNlt01Z2PeU","executionInfo":{"status":"ok","timestamp":1667034858210,"user_tz":-480,"elapsed":1986,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')\n","\n","def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    length = [len(indexed)]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    length_tensor = torch.LongTensor(length)\n","    prediction = torch.sigmoid(model(tensor, length_tensor))\n","    return prediction.item()"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BdHU33oL2PeU"},"source":["An example negative review..."]},{"cell_type":"code","metadata":{"id":"NjYvi35L2PeV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d69940df-9fce-424e-9e35-9b97f4c946ef","executionInfo":{"status":"ok","timestamp":1667034936455,"user_tz":-480,"elapsed":481,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["predict_sentiment(model, \"This film is terrible\")"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3245220482349396"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"hN0uXios2PeV"},"source":["An example positive review..."]},{"cell_type":"code","metadata":{"id":"GjPpyUD12PeV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"553f54d5-d281-40f7-d470-de3823908a47","executionInfo":{"status":"ok","timestamp":1667034940600,"user_tz":-480,"elapsed":511,"user":{"displayName":"Manwai Mak","userId":"13941862130459577180"}}},"source":["predict_sentiment(model, \"This film is great\")"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.849624752998352"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"Na5REOZL2PeV"},"source":["## **4. Further Investigation (Optional)**\n","\n","Try the following:\n","1.   Increase the number of LSTM layer \n","2.   Use uni-directional LSTM\n","3.   Change the embedding dimension\n","3.   Do not use dropout during training\n","\n","Discuss your results and observations in your report. This part is optional. Do it if you have time. It requires full understanding of the program.\n","\n","\n","\n"]}]}