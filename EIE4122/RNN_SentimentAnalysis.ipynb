{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enmwmak/Teaching/blob/main/EIE4122/RNN_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA7et6XT_LVj"
      },
      "source": [
        "# **This Colab is based on the Colab file in [link](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb). To reduce running time, we will use a reduced-size IMDb dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDb6rvdlhN3F"
      },
      "source": [
        "# **Simple Sentiment Analysis**\n",
        "\n",
        "We will be building a machine learning model to detect sentiment (i.e., detecting if a sentence is positive or negative) using PyTorch and TorchText. We will use a reduced-size [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "We will start with a simple model (RNN) to understand the general concepts instead of achieving good performance. In the next Colab, we will build on this knowledge to use more advanced models (Bi-directional LSTM) to obtain better results.\n",
        "\n",
        "### **1. Introduction**\n",
        "\n",
        "We'll be using a **recurrent neural network** (RNN) as they are commonly used in analysing sequences. An RNN takes in sequence of words, $X=\\{x_1, ..., x_T\\}$, one at a time, and produces a _hidden state_, $h$, for each word. We use the RNN _recurrently_ by feeding in the current word $x_t$ as well as the hidden state from the previous word, $h_{t-1}$, to produce the next hidden state, $h_t$. \n",
        "\n",
        "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
        "\n",
        "Once we have our final hidden state $h_T$ (from feeding in the last word in the sequence $x_T$), we feed $h_T$ through a fully-connected linear layer, followed by a sigmoid function $\\sigma()$ to predict the sentiment score $\\hat{y} = \\sigma({\\bf w}^\\top_oh_T)$, where ${\\bf w}_o$ contains the weights of the fully-connected layer at the output. Note that in the program \"imdb-rnn.py\", the sigmoid function is implemented in the function `binary_accuracy()`.\n",
        "\n",
        "The following diagram shows an example sentence, with the RNN predicting zero, which indicates a negative sentiment. The unfolded RNN is shown in orange and the linear layer is shown in silver. Note that we use the same RNN for every word, i.e., it has the same parameters. The initial hidden state, $h_0$, is a tensor initialized to all zeros. \n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment1.png?raw=1)\n",
        "\n",
        "**Note:** some layers and steps have been omitted from the diagram, but these will be explained later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuH3lVDThN3H"
      },
      "source": [
        "### **2. Preparing Data**\n",
        "\n",
        "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task, the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
        "\n",
        "The parameters of a `Field` specify how the data should be processed. We use the `TEXT` field to define how the movie reviews should be processed, and the `LABEL` field determines how the labels should be processed. \n",
        "\n",
        "In \"imdb-rnn.py\", the `TEXT` field uses `tokenize='spacy'` as an argument. This specifies that \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer. If no `tokenize` argument is passed, the default is simply splitting the string on spaces. We also need to specify a `tokenizer_language`, which tells TorchText which spaCy model to use. We use the `en_core_web_sm` model, which has to be downloaded with `python -m spacy download en_core_web_sm` before you run this notebook!\n",
        "\n",
        "`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels. We will explain the `dtype` argument later.\n",
        "\n",
        "For more on `Fields`, go [here](https://pytorch.org/text/_modules/torchtext/data/field.html).\n",
        "\n",
        "We also set the random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5eucG2DpQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66f950d-593e-425c-8542-ac21bccbef2e"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-HO1yf1Wikd",
        "outputId": "14f9987a-e8df-4f5c-8773-8e3f3ccb4bc4"
      },
      "source": [
        "# Create a working folder and cd to it.\n",
        "!mkdir -p /content/drive/MyDrive/Learning/EIE4122/lab2\n",
        "%cd /content/drive/MyDrive/Learning/EIE4122/lab2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Learning/EIE4122/lab2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atdg0a2hmG5z",
        "outputId": "02ceef97-0e47-46d5-9e61-56fdce32fc38"
      },
      "source": [
        "# Require gdown to download files from Google Drive\n",
        "!pip install gdown"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoe02ulpmVTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ffe135-61d1-4678-fc8f-74ae21ed44ba"
      },
      "source": [
        "# Download the reduced-size IMDB dataset\n",
        "!if [ ! -f data.tgz ]; then gdown https://drive.google.com/uc?id=1EL6YlIEs1IbUDjrZvrZ_9-C0C4gYoryZ; fi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EL6YlIEs1IbUDjrZvrZ_9-C0C4gYoryZ\n",
            "To: /content/drive/MyDrive/Learning/EIE4122/lab2/data.tgz\n",
            "100% 67.8M/67.8M [00:00<00:00, 84.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGwk4cI3LYKo",
        "outputId": "aa6c4f41-8634-4b24-d3fe-2a53416e845a"
      },
      "source": [
        "!echo \"Start decompression. It will take few minutes\"\n",
        "!tar zxf data.tgz --exclude=\"._*\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start decompression. It will take few minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPMpWD9ia0di",
        "outputId": "c95f1a61-122f-4953-a5c3-d0169050a01f"
      },
      "source": [
        "# Make sure that decompression is successful. The command \"du\" lists the size of folders.\n",
        "!du -h .data/imdb/aclImdb"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0M\t.data/imdb/aclImdb/test/neg\n",
            "2.2M\t.data/imdb/aclImdb/test/pos\n",
            "25M\t.data/imdb/aclImdb/test\n",
            "2.1M\t.data/imdb/aclImdb/train/neg\n",
            "2.3M\t.data/imdb/aclImdb/train/pos\n",
            "68M\t.data/imdb/aclImdb/train\n",
            "94M\t.data/imdb/aclImdb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download torchtext. It will take a few minutes\n",
        "!pip install torchtext==0.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upmfOlQh7AAb",
        "outputId": "adf8ad60-9fe0-48ca-b76c-c07df9a1de60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.11.0\n",
            "  Downloading torchtext-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.1)\n",
            "Collecting torch==1.10.0\n",
            "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:40tcmalloc: large alloc 1147494400 bytes == 0x38e9e000 @  0x7f023a905615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0 torchtext-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZjfGnBGhN3I"
      },
      "source": [
        "# Define input TEXT and output LABEL\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ChJifgNhN3J"
      },
      "source": [
        "Another handy feature of TorchText is that it has support for common datasets used in natural language processing (NLP). The following code checks whether the IMDb dataset and has been downloaded to '.data/imdb/aclImdb/'. If yes, the code splits the dataset into the canonical train/test splits as `torchtext.datasets` objects. Otherwise, it will automatically download the original dataset from the internet. It processes the data using the `Fields` we have previously defined. \n",
        "\n",
        "The original IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review. However, to speed up training, we will only use 5,560 movie reviews in the original dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K9X7Y2NhN3J"
      },
      "source": [
        "# Because data have been uploaded to \".data/imdb/aclImdb/\". This step will only take 1 minute.\n",
        "# This step should be quick as PyTorch only need to know the structure of the data in the\n",
        "# folder \".data/imdb/aclImdb\". It will not actually load the data, which will be done by\n",
        "# the Dataloader object (see code below)\n",
        "from torchtext.legacy import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, path='.data/imdb/aclImdb/')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Q1_RpBhN3K"
      },
      "source": [
        "We can see how many examples are in each split by checking their length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKo1CvNFhN3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c3860d-528d-4c20-94fe-9b1075f35f3e"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 2780\n",
            "Number of testing examples: 2780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rrBky4hhN3L"
      },
      "source": [
        "We can also check an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-7M0IehhN3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8320fa8-8a54-4ac6-d3ac-7814eec69b77"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['Wow', '!', 'Wow', '!', 'Wow', '!', 'I', 'have', 'never', 'seen', 'a', 'non', '-', 'preachy', 'documentary', 'on', 'globalization', 'until', 'I', 'saw', 'MARDI', 'GRAS', ':', 'MADE', 'IN', 'CHINA', '.', 'This', 'film', 'has', 'zero', 'narration', 'and', 'combines', 'verite', 'footage', 'with', 'sensitive', 'interviews', 'with', 'four', 'teenage', 'workers', 'in', 'China', 'who', 'live', 'inside', 'a', 'factory', 'compound', '.', 'They', 'play', 'with', 'toys', ',', 'jump', 'rope', ',', 'and', 'dance', '.', 'Yet', ',', 'the', 'majority', 'of', 'their', 'days', 'and', 'nights', 'consist', 'of', 'work', ',', 'work', ',', 'and', 'work', '--', 'but', 'the', 'footage', 'of', 'their', 'work', 'is', 'illuminating', 'and', 'mesmerizing', 'to', 'watch', '.', 'The', 'owner', 'of', 'the', 'factory', 'in', 'China', 'is', 'amazingly', 'open', ',', 'so', 'much', 'so', 'that', 'he', 'hits', 'home', 'the', 'effects', 'of', 'globalization', 'while', 'he', '\"', 'punishes', '\"', 'the', 'workers', '.', 'Astutely', 'following', 'Mardi', 'Gras', 'beads', 'from', 'China', 'to', 'the', 'Carnival', ',', 'the', 'film', 'reveals', 'how', 'the', 'local', 'is', 'connected', 'to', 'the', 'global', 'through', 'humor', 'and', 'interesting', ',', 'compelling', 'footage', 'from', 'both', 'cultures', '.', 'One', 'of', 'the', 'most', 'interesting', 'parts', 'in', 'this', 'film', 'is', 'the', 'cross', 'cultural', 'introduction', 'of', 'factory', 'workers', 'and', 'Mardi', 'Gras', 'revelers', 'to', 'each', 'other', 'through', 'pictures', '.', 'Here', ',', 'the', 'film', 'comes', 'full', 'circle', 'and', 'shows', 'how', 'images', 'can', 'be', 'a', 'point', 'of', 'communication', 'and', 'transformation', '.', 'The', 'film', 'is', 'never', 'preachy', ',', 'is', 'not', 'guilt', 'driven', ',', 'and', 'allows', 'everyone', \"'s\", 'point', 'of', 'view', 'to', 'be', 'present', '.', 'At', 'the', 'end', ',', 'we', '--', 'the', 'viewers', '--', 'make', 'up', 'our', 'own', 'conclusions', 'about', 'the', 'complexity', 'of', 'the', 'film', ',', 'and', 'globalization', '.'], 'label': 'pos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wq_TApghN3L"
      },
      "source": [
        "The IMDb dataset only has train/test splits, so we need to create a validation set. We can do this with the `.split()` method. \n",
        "\n",
        "By default this splits data into 70/30. However, by passing a `split_ratio` argument, we can change the split ratio, i.e., a `split_ratio` of 0.8 would mean 80% of the examples make up the training set and 20% make up the validation set. \n",
        "\n",
        "We also pass our random seed to the `random_state` argument to ensure that we get the same train/validation split each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CuKM5B1hN3M"
      },
      "source": [
        "import random\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gnq6ac5hN3M"
      },
      "source": [
        "Again, we can see how many examples are in each split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3q4fJrzhN3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412d3f9d-97d5-4279-99b7-d0b00913eb47"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 1946\n",
            "Number of validation examples: 834\n",
            "Number of testing examples: 2780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAyNHsSWhN3N"
      },
      "source": [
        "Next, we have to build a _vocabulary_. This is effectively a look up table where every unique word in our data set has a corresponding _index_ (an integer). We need to do this because our machine learning model cannot operate on strings, only numbers. \n",
        "\n",
        "Each _index_ is used to construct a _one-hot_ vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and the dimensionality is the total number of unique words in our vocabulary, commonly denoted by $V$.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment5.png?raw=1)\n",
        "\n",
        "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit into our GPU. There are two effective ways to cut down our vocabulary: (1) only take the top $n$ most common words and (2) ignore words that appear less than $m$ times. We'll use the former, i.e., only keeping the top 25,000 words.\n",
        "\n",
        "What do we do with words that are ignored? We may replace them with a special _unknown_ `<unk>` token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I `<unk>` it\".\n",
        "\n",
        "The following builds the vocabulary, only keeping the most common `max_size` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMttoY6VhN3N"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000    # In Python 25_000 means 25000\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-aoMnzPhN3N"
      },
      "source": [
        "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_11cRTEyhN3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a470d47-7539-4e69-ea8a-ded2d598cbf4"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qupsJLRFhN3O"
      },
      "source": [
        "Why is the vocab size 25002 and not 25000? One of the addition tokens is the `<unk>` token and the other is a `<pad>` token.\n",
        "\n",
        "When we feed sentences into our model, we feed a _batch_ of them at a time, i.e., more than one at a time, and all sentences in the batch need to be of the same size. Thus, to ensure that each sentence in the batch is of the same size, any sentences shorter than the longest within the batch are padded.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment6.png?raw=1)\n",
        "\n",
        "We can also view the most common words in the vocabulary and their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZeO8Xn1hN3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8f4cc7-aafc-4d44-e30e-6f839da4842e"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 21892), (',', 21224), ('.', 18358), ('a', 12304), ('and', 12252), ('of', 10946), ('to', 10386), ('is', 8618), ('in', 6804), ('I', 5771), ('it', 5732), ('that', 5346), ('\"', 5135), (\"'s\", 4792), ('this', 4555), ('-', 4282), ('was', 3872), ('/><br', 3819), ('as', 3400), ('with', 3357)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJsJcaNAhN3P"
      },
      "source": [
        "We can also see the vocabulary directly using either the `stoi` (**s**tring **to** **i**nt) or `itos` (**i**nt **to**  **s**tring) method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCcpmZKAhN3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e64fec6-05dc-4b34-c544-2b16aa07006e"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])      # Print from the beginning (0th) to the 9th position"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpElXQDYhN3Q"
      },
      "source": [
        "We can also check the labels, ensuring 0 is for negative and 1 is for positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92P1Qwf_hN3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa771192-0fc8-45fd-9d51-fc8b4ab04e32"
      },
      "source": [
        "print(LABEL.vocab.stoi)\n",
        "print(LABEL.vocab.stoi['neg'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(None, {'neg': 0, 'pos': 1})\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQPt_pJehN3Q"
      },
      "source": [
        "The final step of preparing the data is creating the iterators. We use the iterators to iterate over the training/validation sets in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "\n",
        "We'll use a `BucketIterator` which is a special type of iterator that will return a batch of examples where each example is of a similar length. This can minimize the amount of padding per example.\n",
        "\n",
        "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using `torch.device`. We then pass this device to the iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMYQ0yY_hN3Q"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRUsjZ8UhN3R"
      },
      "source": [
        "### **3. Build the Model**\n",
        "\n",
        "The next stage is building the model that we'll eventually train and evaluate. There is a small amount of boilerplate code when creating models in PyTorch. Note that our `RNN` class is a sub-class of `nn.Module` and that we use `super` to call the superclass's `__init__` method.\n",
        "\n",
        "Within the `__init__` we define the _layers_ of the module. The three layers comprises an _embedding_ layer, an _RNN_ layer, and a _linear_ layer. All layers have their parameters initialized to random values, unless explicitly specified.\n",
        "\n",
        "The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector (dense as the dimensionality is a lot smaller and all the elements are real numbers). This embedding layer is simply a single fully connected layer. In addition to reducing the dimensionality of the input to the RNN layer, there is the theory that words which have similar impact on the sentiment of the review are mapped close together in this dense vector space. For more information about word embeddings, see [here](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
        "\n",
        "The RNN layer takes in our dense vector and the previous hidden state $h_{t-1}$, which it uses to calculate the next hidden state, $h_t$.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment7.png?raw=1)\n",
        "\n",
        "Finally, the linear layer takes the final hidden state and feeds it through a fully-connected layer ${\\bf w}_0^\\top h_T$, transforming it to the correct output dimension. In our case, the output dimension is 1. The `forward` method is called when we feed examples into our model.\n",
        "\n",
        "Each batch, `text`, is a tensor of size _**[sentence length, batch size]**_. That is a batch of sentences, each having each word converted into a one-hot vector. \n",
        "\n",
        "You may notice that this tensor should have another dimension due to the one-hot vectors, however PyTorch conveniently stores a one-hot vector as it's index value, i.e., the tensor representing a sentence is just a tensor of the indexes for each token in that sentence. The act of converting a list of tokens into a list of indexes is commonly called *numericalizing*.\n",
        "\n",
        "The input batch is then passed through the embedding layer to get `embedded`, which gives us a dense vector representation of our sentences. `embedded` is a tensor of size _**[sentence length, batch size, embedding dim]**_.\n",
        "\n",
        "`embedded` is then fed into the RNN layer. In some frameworks you must feed the initial hidden state, $h_0$, into the RNN, however in PyTorch, if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
        "\n",
        "The RNN returns 2 tensors, `output` of size _**[sentence length, batch size, hidden dim]**_ and `hidden` of size _**[1, batch size, hidden dim]**_. `output` is the concatenation of the hidden state from every time step, whereas `hidden` is simply the final hidden state. We verify this using the `assert` statement. Note the `squeeze` method, which is used to remove a dimension of size 1. \n",
        "\n",
        "Finally, we feed the last hidden state, `hidden`, through the linear layer, `fc`, to produce a prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouy6f2PxhN3R"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)        \n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)      \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        #text = [sent len, batch size]        \n",
        "        \n",
        "        embedded = self.embedding(text)        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)        \n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))  # Remove the first dim in hidden to return [batch_size, hid_dim]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbgW3WdKhN3S"
      },
      "source": [
        "We now create an instance of our RNN class. The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size. The embedding dimension is the size of the dense word vectors. This is usually around 50-250 dimensions, but depends on the size of the vocabulary.\n",
        "The hidden dimension is the size of the hidden states. This is usually around 100-500 dimensions, but also depends on factors such as the vocabulary size, the size of the dense vectors and the complexity of the task. The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional, i.e., a single scalar real number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAAVZsrEhN3S"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOYcV4evhN3S"
      },
      "source": [
        "Let's also create a function that will tell us how many trainable parameters our model has so we can compare the number of parameters across different models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCz-wugbhN3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c66efed-3398-4ba4-8700-c1bbaa1890a3"
      },
      "source": [
        "# Count the number of trainable parameters in our model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 2,592,105 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfmFeFyNhN3T"
      },
      "source": [
        "### **4. Train the Model**\n",
        "Now we'll set up the training and then train the model. First, we'll create an optimizer. This is the algorithm we use to update the parameters of the module. Here, we'll use _stochastic gradient descent_ (SGD). The first argument is the parameters that will be updated by the optimizer. The second is the learning rate, i.e., how much we'll change the parameters when we do a parameter update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjV4DFiRhN3V"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWf5A6xBhN3V"
      },
      "source": [
        "Next, we'll define our loss function. In PyTorch, this is commonly called a criterion. The loss function here is _binary cross entropy with logits_. \n",
        "\n",
        "Our model currently outputs an unbound real number. As our labels are either 0 or 1, we want to restrict the predictions to a number between 0 and 1. We do this using the _sigmoid_ or _logit_ functions. We then use this this bound scalar to calculate the loss using binary cross entropy. The `BCEWithLogitsLoss` criterion carries out both the sigmoid and the binary cross entropy steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7YM6fqchN3X"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in6mb3GDhN3X"
      },
      "source": [
        "Using `.to`, we can place the model and the criterion on the GPU (if we have one). \n",
        "<font color='red'>If this step takes a long period. Re-execute the statements may help.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PvTGEv1hN3Y"
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv3cBMtOhN3Y"
      },
      "source": [
        "Our criterion function calculates the loss. However, we have to write our function to calculate the accuracy. This function first feeds the predictions through a sigmoid layer to squash the values between 0 and 1. Then, we round them to the nearest integer, i.e., rounding any value greater than 0.5 to 1 (a positive sentiment) and the rest to 0 (a negative sentiment). We then calculate how many rounded predictions equal the actual labels and average it across the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gxERCHhhN3Y"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl9e6n65hN3Y"
      },
      "source": [
        "The `train` function iterates over all examples, one batch at a time. `model.train()` is used to put the model in \"training mode\", which turns on _dropout_ and _batch normalization_. Although we aren't using them in this model, it's good practice to include it.\n",
        "\n",
        "For each batch, we first zero the gradients. Each parameter in a model has a `grad` attribute which stores the gradient calculated by the `criterion`. PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
        "\n",
        "We then feed the batch of sentences, `batch.text`, into the model. Note, you do not need to do `model.forward(batch.text)`. Instead, simply calling the model will do. The `squeeze` is needed as the predictions are initially size _**[batch size, 1]**_, and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size _**[batch size]**_.\n",
        "\n",
        "The loss and accuracy are then calculated using our predictions and the labels, `batch.label`, with the loss being averaged over all examples in the batch.\n",
        "\n",
        "We calculate the gradient of each parameter with `loss.backward()`, and then update the parameters using the gradients and optimizer algorithm with `optimizer.step()`.\n",
        "The loss and accuracy is accumulated across the epoch, the `.item()` method is used to extract a scalar from a tensor which only contains a single value. Finally, we return the loss and accuracy, averaged across the epoch. The `len` of an iterator is the number of batches in the iterator.\n",
        "\n",
        "You may recall when initializing the `LABEL` field, we set `dtype=torch.float`. This is because TorchText sets tensors to be `LongTensor` by default, however our criterion expects both inputs to be `FloatTensor`. Setting the `dtype` to be `torch.float`, did this for us. The alternative method of doing this would be to do the conversion inside the `train` function by passing `batch.label.float()` instad of `batch.label` to the criterion. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuLo22nphN3Z"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, max_samples=2500):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    #for batch in iterator:\n",
        "    for i, batch in enumerate(iterator):\n",
        "        if i > max_samples:                               # Use less samples to reduce training time\n",
        "            break  \n",
        "        optimizer.zero_grad()                             # Initialize gradient to 0 for current batch\n",
        "        predictions = model(batch.text).squeeze(1)        # Call RNN.forward() and get return object\n",
        "        loss = criterion(predictions, batch.label)        # Compute binary cross-entropy\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        loss.backward()                                   # Compute gradient\n",
        "        optimizer.step()                                  # Update the weights by gradient descent\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzDQ8cSXhN3a"
      },
      "source": [
        "`evaluate` is similar to `train`, with a few modifications as you don't want to update the parameters when evaluating.\n",
        "`model.eval()` puts the model in \"evaluation mode\", which turns off _dropout_ and _batch normalization_. Again, we are not using them in this model, but it is good practice to include them.\n",
        "\n",
        "No gradients are calculated on PyTorch operations inside the `with no_grad()` block. This causes less memory to be used and speeds up computation.\n",
        "\n",
        "The rest of the function is the same as `train`, with the removal of `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`, as we do not update the model's parameters when evaluating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E96Xue17hN3a"
      },
      "source": [
        "def evaluate(model, iterator, criterion, max_samples=2500):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():                                    # No gradient will be needed\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text).squeeze(1)       # Call RNN.forward()           \n",
        "            loss = criterion(predictions, batch.label)       # Compute binary cross entropy loss\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO-qxPsZhN3a"
      },
      "source": [
        "We'll also create a function to tell us how long an epoch takes to compare training times between models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3vu6477hN3b"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be0hy3S6hN3b"
      },
      "source": [
        "We then train the model through multiple epochs, an epoch being a complete pass through all examples in the training and validation sets. At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An-rmkQ5hN3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57409e7a-f98d-4f47-accc-35a86f8629f5"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.697 | Train Acc: 49.65%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 50.33%\n",
            "Epoch: 02 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.694 | Train Acc: 49.52%\n",
            "\t Val. Loss: 0.695 |  Val. Acc: 50.33%\n",
            "Epoch: 03 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.694 | Train Acc: 48.81%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 50.33%\n",
            "Epoch: 04 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.693 | Train Acc: 48.91%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 50.45%\n",
            "Epoch: 05 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.58%\n",
            "\t Val. Loss: 0.696 |  Val. Acc: 45.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBstiHjZhN3b"
      },
      "source": [
        "You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we'll improve in the next notebook.\n",
        "\n",
        "Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYcc_lsXhN3c",
        "outputId": "133ad314-27b6-44a7-d403-cdf410465654",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.698 | Test Acc: 49.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En067uP2hN3c"
      },
      "source": [
        "### **5. Further Investigations**\n",
        "\n",
        "Try the following:\n",
        "1.   Increase the number of RNN layers \n",
        "2.   Change the embedding dimension of your RNN\n",
        "3.   Reduce the dimension of word embedding\n",
        "\n",
        "Discuss your results and observations in your report. Also, determine the type of loss function used by the RNN in this program."
      ]
    }
  ]
}