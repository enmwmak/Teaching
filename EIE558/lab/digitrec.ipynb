{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fszgXmA8MZY"
   },
   "source": [
    "**EIE558 Speech Recognition Lab (Part 1): Spoken-Digit Recognition**\n",
    "\n",
    "In this lab, you will train and evaluate a CNN model that comprises several 1-D CNN layers for spoken digit recognition. By default, the input to the CNN is an MFCC matrix of size *C* x *T*, where *C* is the number MFCC coefficients per frame and *T* is the number of frames. \n",
    "\n",
    "Two pooling methods are available for converting frame-based features to utterance-based features. They are adaptive average pooling and statistics pooling. The former uses PyTorch's AdaptiveAvgPooling2d() to average the last convolutional layer's activation across the frame axis. The latter concatenates the mean and the standard deviation of the activations across frames, which is commonly used in the x-vector network. If no pooling method is used, the number of frames for each utterance should be the same so that the number of nodes after flattening is identical for all utterances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "186SdUa78s9A"
   },
   "source": [
    "<font color=\"green\">*Step 1: Prepare environment*<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzQNackhq703",
    "outputId": "e21fd625-6f80-47d9-920d-5461e67b50f1"
   },
   "outputs": [],
   "source": [
    "# If you use Colab, run this cell to mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!mkdir -p /content/drive/MyDrive/Learning/EIE558/asr\n",
    "%cd /content/drive/MyDrive/Learning/EIE558/asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the version of PyTorch\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41E_3gSuyScL"
   },
   "source": [
    "<font color=\"green\">*Step 2: Download programs and data. If the 'python-asr' directory exists and is empty, you may delete the 'python-asr' directory and run this step again.*<font> <font color=\"red\">*In case the website http://bioinfo.eie.polyu.edu.hk is too slow or busy, you may find the files [here](https://polyuit-my.sharepoint.com/:f:/g/personal/enmwmak_polyu_edu_hk/EpX3v5ykT_VLoiBa8jrpJ70B52X4XbEPQcyrDnLAquEcIA?e=d5Xjrv)<font>*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kh-jUQpD7gde",
    "outputId": "aca10dc6-2d3c-4112-a7a4-5fa42b5dd09f"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pwd\n",
    "dir=\"python-asr\"\n",
    "if [ ! -d \"$dir\" ]; then\n",
    "  echo \"Directory $dir does not exist. Downloading ${dir}.tgz\"\n",
    "  wget http://bioinfo.eie.polyu.edu.hk/download/EIE558/asr/${dir}.tgz;\n",
    "  tar zxf ${dir}.tgz;\n",
    "  rm -f ${dir}.tgz*;\n",
    "else\n",
    "  echo \"Directory $dir already exist\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this notebook file on your own computer, run this cell\n",
    "%cd python-asr\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzoJNd6PmKt8",
    "outputId": "fe74fd91-e0ba-4896-98cf-b81ea941d1ce"
   },
   "outputs": [],
   "source": [
    "# If you run this notebook file on Colab, run this cell\n",
    "%cd /content/drive/MyDrive/Learning/EIE558/asr/python-asr/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">*Download datasets (532 MBytes). If the 'data' directory exists and is empty, \n",
    "you may delete the 'data' directory and run this step again.*<font> <font color=\"red\">*In case the website http://bioinfo.eie.polyu.edu.hk is too slow or busy, you may find the files [here](https://polyuit-my.sharepoint.com/:f:/g/personal/enmwmak_polyu_edu_hk/EpX3v5ykT_VLoiBa8jrpJ70B52X4XbEPQcyrDnLAquEcIA?e=d5Xjrv) This step will take a while.<font>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-ToOSCe6zej"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dir=\"data\" \n",
    "if [ ! -d $dir ]; then\n",
    "  echo \"Directory $dir does not exist. Downloading ${dir}.zip\"\n",
    "  wget http://bioinfo.eie.polyu.edu.hk/download/EIE558/asr/${dir}.zip;\n",
    "  unzip -o ${dir}.zip;\n",
    "  rm -f ${dir}.zip*;\n",
    "else\n",
    "  echo \"Directory $dir already exist\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub383ypr9D5n"
   },
   "source": [
    "<font color=\"green\">*Step 3: Train a CNN model. It may take several hours to train a model if you use all of the training data in the list file \"data/digits/train.lst\". You may want to use the pre-trained models in the folder \"models/\" if you want to obtain test accuracy only. Read the file \"digitrec.py\" and \"model.py\" to see how to implement a CNN for spoken digit recognition. If you want to train your own models, you may modify the file \"digitrec.py such that \"data/digits/train.lst\" is replaced by \"data/digits/short_train.lst\" and \"data/digits/test.lst\" is replaced by data/digits/short_test.lst\". With these modifications, it will take about 30 minutes to train a network. But the accuracy is lower.*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you are still under the folder 'python-asr'\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkf5va9trFc_",
    "outputId": "93135769-566b-42be-f933-537b60cb3930"
   },
   "outputs": [],
   "source": [
    "# Create reduced training and test set to reduce training and test time\n",
    "!more data/digits/train.lst | sed -n '1,2000p' > data/digits/short_train.lst\n",
    "!more data/digits/test.lst | sed -n '1,500p' > data/digits/short_test.lst\n",
    "!mkdir -p models/mymodels\n",
    "!python3 digitrec.py --pool_method stats --model_file models/mymodels/spokendigit_cnn_stats.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4_dg4YTBuFk"
   },
   "source": [
    "<font color=\"green\">*Step 4: Load the trained model (or the pre-trained model) and evaluate it*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKLgd6c2SmZy"
   },
   "outputs": [],
   "source": [
    "# Define the prediction function, using a DataLoader object that comprises \n",
    "# the test data as input\n",
    "from digitrec import get_default_device\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_dl(model, dl):\n",
    "    device = get_default_device()\n",
    "    torch.cuda.empty_cache()\n",
    "    batch_probs = []\n",
    "    batch_targ = []\n",
    "    for xb, yb in dl:\n",
    "        xb = xb.float().to(device)\n",
    "        yb = yb.float().to(device)\n",
    "        probs = model(xb)\n",
    "        batch_probs.append(probs.cpu().detach())\n",
    "        batch_targ.append(yb.cpu().detach())\n",
    "    batch_probs = torch.cat(batch_probs)\n",
    "    batch_targ = torch.cat(batch_targ)\n",
    "    return [list(values).index(max(values)) for values in batch_probs], batch_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdsBM4z3B64-",
    "outputId": "333d522d-7cb0-4230-88a1-db65c88418de"
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "from digitrec import get_default_device\n",
    "from model import CNNModel\n",
    "device = get_default_device()\n",
    "model = CNNModel(pool_method='stats').to(device)\n",
    "model.load_state_dict(torch.load('models/mymodels/spokendigit_cnn_stats.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_y7KtoGpPLU",
    "outputId": "b1023eda-10d6-4932-af4e-7d516c8d4a6a"
   },
   "outputs": [],
   "source": [
    "# Evaluate the loaded model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from digitrec import SpeechDataset, evaluate\n",
    "test_set = SpeechDataset(filelist='data/digits/short_test.lst', rootdir='data/digits', n_mfcc=20)\n",
    "test_dl = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
    "r = evaluate(model, test_dl)\n",
    "yp, yt = predict_dl(model, test_dl)\n",
    "print(\"Loss: \", r['loss'], \"\\nAccuracy: \", r['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LG5WjvpDNiPn"
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained model that uses statistics pooling in its embedding layer.\n",
    "from digitrec import get_default_device\n",
    "from model import CNNModel\n",
    "device = get_default_device()\n",
    "model = CNNModel(pool_method='stats').to(device)\n",
    "model.load_state_dict(torch.load('models/spokendigit_cnn_stats.pth', \n",
    "                                 map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFvt-DWuN9YA"
   },
   "outputs": [],
   "source": [
    "# Evaluate the loaded model\n",
    "r = evaluate(model, test_dl)\n",
    "yp, yt = predict_dl(model, test_dl)\n",
    "print(\"Loss: \", r['loss'], \"\\nAccuracy: \", r['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model that uses adaptive average pooling in its embedding layer.\n",
    "model = CNNModel(pool_method='adapt').to(device)\n",
    "model.load_state_dict(torch.load('models/spokendigit_cnn_adapt.pth', \n",
    "                                 map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the loaded model\n",
    "r = evaluate(model, test_dl)\n",
    "yp, yt = predict_dl(model, test_dl)\n",
    "print(\"Loss: \", r['loss'], \"\\nAccuracy: \", r['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model that use flattening in its embedding layer.\n",
    "model = CNNModel(pool_method='none').to(device)\n",
    "model.load_state_dict(torch.load('models/spokendigit_cnn_none.pth', \n",
    "                                 map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the loaded model\n",
    "r = evaluate(model, test_dl)\n",
    "yp, yt = predict_dl(model, test_dl)\n",
    "print(\"Loss: \", r['loss'], \"\\nAccuracy: \", r['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">*Explain the performance difference between (1) CNN with statistics pooling, (2) CNN with average pooling, and (3) CNN with flattening*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nudpBj4Zkemr"
   },
   "source": [
    "<font color=\"green\">*Step 5: Varying the kernel size. Increase the kernel size in \"model.py\" to 7 (or even larger) and repeat Step 4. Record the test loss and accuracy. Reduce the kernel size to 1 and observe the results. Can the CNN still capture the temporal characteristics in the MFCCs when kernel_size=1? Explain your answer.*</font> <font color=\"red\">*If the model remains unchanged even after you have saved the file \"model.py\", you may reset the runtime by selecting \"Runtime\", followed by \"Reset runtime\".*</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNudrUfu49vW"
   },
   "source": [
    "<font color=\"green\">*Step 6: Reduce the depth of the network so that the conv2, conv3, and conv4 in \"model.py\" are removed. After the change, the network only have one convolutional layer. Observe the performance of the network. Note that large and deep networks may not necessary produce better results, especially when the amount of training data is limited.*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDjDGSNEOAPh",
    "outputId": "39d92b23-c97a-4144-d6e0-b3a59050bf9b"
   },
   "outputs": [],
   "source": [
    "from model import CNNModel\n",
    "model = CNNModel(pool_method='adapt')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 digitrec.py --pool_method stats --model_file models/mymodels/spokendigit_resnet_stats.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1HPmkNS36wEv",
    "Io-WhKnHUh6P"
   ],
   "name": "digitrec.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "27c8649a8deb437c435a6e7d76746b364b3b24f1d91deffadd606e2cd59c0f47"
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
