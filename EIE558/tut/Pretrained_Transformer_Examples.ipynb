{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzhv10/1Bn7cOQpIN7t3ef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enmwmak/Teaching/blob/main/EIE558/tut/Pretrained_Transformer_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation"
      ],
      "metadata": {
        "id": "i-I_MG1bfRgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMzpMYTBKtwR"
      },
      "outputs": [],
      "source": [
        "# Install the necessary library\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the installation\n",
        "import torch\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "GS0kdoESL4Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "YK5bmHDEfd3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load a pre-trained sentiment analysis model\n",
        "# The 'sentiment-analysis' pipeline automatically loads a suitable model and tokenizer.\n",
        "# You can specify a particular model if desired, e.g., 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "#classifier = pipeline('sentiment-analysis')\n",
        "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "# 2. Prepare your input text\n",
        "text_to_analyze = [\n",
        "    \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
        "    \"The service was terrible and I'm very disappointed.\",\n",
        "    \"It was an okay experience, nothing special.\"\n",
        "]\n",
        "\n",
        "# 3. Perform sentiment analysis\n",
        "results = classifier(text_to_analyze)\n",
        "\n",
        "# 4. Display the results\n",
        "print(\"Sentiment Analysis Results:\")\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Text: \\\"{text_to_analyze[i]}\\\"\")\n",
        "    print(f\"Label: {result['label']}, Score: {result['score']:.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "xYvIA_NEK0aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of using a specific model for a different task (e.g., masked language modeling)\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# Load a pre-trained BERT model and tokenizer for masked language modeling\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Create an input sequence with a masked token\n",
        "text_mlm = \"The capital of France is [MASK].\"\n",
        "inputs = tokenizer(text_mlm, return_tensors=\"pt\")\n",
        "\n",
        "# Predict the masked token\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "masked_token_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
        "predicted_token_id = logits[0, masked_token_index].argmax(axis=-1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f\"Predicted masked token for \\\"{text_mlm}\\\": {predicted_token}\")"
      ],
      "metadata": {
        "id": "CatFk_R-L2rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation"
      ],
      "metadata": {
        "id": "dZ-Ma74mfkSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 1. Initialize the text-generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "# 2. Define a text prompt to start the generation\n",
        "prompt = \"Briefly outline the history of China.\"\n",
        "\n",
        "# 3. Generate text from the prompt\n",
        "#   max_new_tokens: The maximum number of new tokens to generate.\n",
        "#   num_return_sequences: How many different sequences to generate.\n",
        "generated_text = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=50,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "# 4. Print the generated text\n",
        "for i, sequence in enumerate(generated_text):\n",
        "    print(f\"Generated Sequence {i+1}: {sequence['generated_text']}\\n\")"
      ],
      "metadata": {
        "id": "qUkpOBh9fFGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text-to-speech Synthesis"
      ],
      "metadata": {
        "id": "_GMClwVuXKkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==3.6.0"
      ],
      "metadata": {
        "id": "DX_cgQaYWdEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
        "from datasets import load_dataset\n",
        "from IPython.display import Audio\n",
        "\n",
        "# 1. Initialize the processor and text-to-speech model\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "# 2. Define the text to be synthesized\n",
        "text = \"Hello, my friends. I am a Speech T5 model, and I can convert your text into speech.\"\n",
        "\n",
        "# 3. Process the text and get the model inputs\n",
        "inputs = processor(text=text, return_tensors=\"pt\")\n",
        "\n",
        "# 4. Load a speaker embedding\n",
        "# A speaker embedding helps the model synthesize speech in a specific voice.\n",
        "# Here, we load an x-vector embedding from a dataset for a male speaker.\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "# 5. Generate the speech waveform\n",
        "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
        "\n",
        "# 6. Play the generated audio\n",
        "Audio(speech.numpy(), rate=16000)"
      ],
      "metadata": {
        "id": "WCMFSsbpVFCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech Feature Extraction"
      ],
      "metadata": {
        "id": "VJKz87LDYZYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already installed\n",
        "!pip install transformers datasets torchaudio librosa"
      ],
      "metadata": {
        "id": "5_p05PdVYER3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load a pre-trained model and feature extractor\n",
        "# We'll use a Wav2Vec2 model pre-trained for speech recognition,\n",
        "# but we'll focus on extracting features before the classification head.\n",
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# 2. Load a sample audio dataset\n",
        "# For demonstration, we'll use a small portion of the LibriSpeech dataset.\n",
        "dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\", streaming=True)\n",
        "sample_audio = next(iter(dataset))\n",
        "audio_array = sample_audio[\"audio\"][\"array\"]\n",
        "sampling_rate = sample_audio[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "# Resample if necessary (Wav2Vec2 typically expects 16kHz)\n",
        "if sampling_rate != feature_extractor.sampling_rate:\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=feature_extractor.sampling_rate)\n",
        "    audio_array = resampler(torch.tensor(audio_array)).numpy()\n",
        "    sampling_rate = feature_extractor.sampling_rate\n",
        "\n",
        "# 3. Extract features\n",
        "# The feature extractor preprocesses the audio for the model.\n",
        "# We'll get the raw hidden states from the model's encoder, which serve as features.\n",
        "inputs = feature_extractor(audio_array, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Pass the input through the model, but we're interested in the encoder's output\n",
        "    # By default, AutoModelForAudioClassification will output logits,\n",
        "    # but we can access intermediate hidden states.\n",
        "    # The output structure might vary slightly between models.\n",
        "    # For Wav2Vec2, 'hidden_states' will contain the outputs of each encoder layer.\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "    # Typically, the last hidden state of the encoder is used for feature extraction.\n",
        "    speech_features = outputs.hidden_states[-1]\n",
        "\n",
        "print(f\"Shape of extracted speech features: {speech_features.shape}\")\n",
        "\n",
        "# 4. Visualize a portion of the extracted features (optional)\n",
        "# We'll visualize the first dimension of the features over time.\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(speech_features[0, :, 0].numpy())\n",
        "plt.title(\"Extracted Speech Features (First Dimension)\")\n",
        "plt.xlabel(\"Time Steps\")\n",
        "plt.ylabel(\"Feature Value\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# You can now use 'speech_features' for downstream tasks like\n",
        "# speaker recognition, emotion recognition, or other audio analysis."
      ],
      "metadata": {
        "id": "cdWwoZlxYe-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot features as a spectrogram\n",
        "import librosa.display\n",
        "plt.subplot(211)\n",
        "librosa.display.specshow(librosa.amplitude_to_db(speech_features[0,:,:]).T,\n",
        "                         sr=sampling_rate, y_axis='linear', hop_length=int(0.01*sampling_rate))\n",
        "plt.subplot(313)\n",
        "librosa.display.waveshow(audio_array, sr=sampling_rate, offset=0)\n",
        "plt.margins(x=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4XzuG3d1Zrxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Translation"
      ],
      "metadata": {
        "id": "fAI9WsR_h3Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pl = pipeline(\n",
        "    task=\"text2text-generation\",\n",
        "    model=\"google-t5/t5-base\",\n",
        "    dtype=torch.float16,\n",
        "    device=0\n",
        ")\n",
        "pl(\"Translate English to French: The weather is nice today.\")"
      ],
      "metadata": {
        "id": "1pavnAYKcf0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27091e76-4b13-4e8b-f38a-138384191d90"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Le temps est agréable aujourd'hui.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pl(\"Translate English to French: Electrical and Electronic Engineering.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO2ZHpAfekn0",
        "outputId": "0985f56b-506e-4212-d21d-9a32a0d13869"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Génie électrique et électronique.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}